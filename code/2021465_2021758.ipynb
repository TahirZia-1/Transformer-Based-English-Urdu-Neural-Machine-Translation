{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0d2c8f49",
      "metadata": {
        "id": "0d2c8f49"
      },
      "source": [
        "# Muhammad Tahir Zia - 2021465\n",
        "\n",
        "# Akhtar Ali - 2021758"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZV2qWl9oWHp8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV2qWl9oWHp8",
        "outputId": "817890f9-daa4-40d9-9aa8-2d319d01e310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iVtUYmQpZER0",
      "metadata": {
        "id": "iVtUYmQpZER0"
      },
      "source": [
        "### Downloaded Compatible Versions for Torch and TorchText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SYirWU-JWI-1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYirWU-JWI-1",
        "outputId": "121e7e0f-dc23-4dcd-e914-0acf60781742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==2.0.0\n",
            "  Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchtext==0.15.1\n",
            "  Downloading torchtext-0.15.1-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (2.0.2)\n",
            "Collecting torchdata==0.6.0 (from torchtext==0.15.1)\n",
            "  Downloading torchdata-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (892 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.4.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.15.1-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchdata-0.6.0 torchtext-0.15.1 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install torch==2.0.0 torchtext==0.15.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MNdb4ZKFZOmW",
      "metadata": {
        "id": "MNdb4ZKFZOmW"
      },
      "source": [
        "### Created an environment to Train the Model otherwise it was giving errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65M2M-_0Vjgt",
      "metadata": {
        "id": "65M2M-_0Vjgt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "import torch\n",
        "torch.use_deterministic_algorithms(True)\n",
        "# Set up an environment for training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WBHfuavDZdxL",
      "metadata": {
        "id": "WBHfuavDZdxL"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323cca6e",
      "metadata": {
        "id": "323cca6e"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from typing import Optional, Any, Union, Callable\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import time\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Module\n",
        "from torch.nn import MultiheadAttention\n",
        "from torch.nn import ModuleList\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import Linear\n",
        "from torch.nn import LayerNorm\n",
        "\n",
        "import spacy\n",
        "\n",
        "from collections import Counter\n",
        "import io\n",
        "from torchtext.vocab import vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1fe6281",
      "metadata": {
        "id": "e1fe6281"
      },
      "source": [
        "## Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c8e72db",
      "metadata": {
        "id": "8c8e72db"
      },
      "source": [
        "### Urdu Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd166a3e",
      "metadata": {
        "id": "cd166a3e"
      },
      "outputs": [],
      "source": [
        "# load urdu object via spacy\n",
        "nlp = spacy.blank('ur')\n",
        "# generate token for input urdu text\n",
        "def urdu_sen_tokens(inp):\n",
        "    doc=nlp(inp)\n",
        "    doc=str(doc)\n",
        "    doc=doc.replace(\"\\n\",\"\")\n",
        "    return [doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80ba7679",
      "metadata": {
        "id": "80ba7679"
      },
      "source": [
        "### English Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a30c692",
      "metadata": {
        "id": "6a30c692"
      },
      "outputs": [],
      "source": [
        "#loading english object from spacy\n",
        "nlp = spacy.blank('en')\n",
        "# generate token for input english text\n",
        "def eng_sen_tokens(inp):\n",
        "    doc=nlp(inp)\n",
        "    doc=str(doc)\n",
        "    doc=doc.replace(\"\\n\",\"\")\n",
        "    return [doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2ed786",
      "metadata": {
        "id": "6c2ed786"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70df38e6",
      "metadata": {
        "id": "70df38e6"
      },
      "source": [
        "### Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9733e43",
      "metadata": {
        "id": "f9733e43"
      },
      "source": [
        "#### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf7b9d6f",
      "metadata": {
        "id": "bf7b9d6f"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(Module):\n",
        "\n",
        "    __constants__ = ['batch_first', 'norm_first']\n",
        "\n",
        "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
        "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
        "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
        "                 device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "                                            **factory_kwargs)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)#input features,output features\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
        "\n",
        "        self.norm_first = norm_first\n",
        "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        # Legacy string support for activation function.\n",
        "        if isinstance(activation, str):\n",
        "            activation = _get_activation_fn(activation)\n",
        "\n",
        "        if activation is F.relu:\n",
        "            self.activation_relu_or_gelu = 1\n",
        "        elif activation is F.gelu:\n",
        "            self.activation_relu_or_gelu = 2\n",
        "        else:\n",
        "            self.activation_relu_or_gelu = 0\n",
        "        self.activation = activation\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "\n",
        "        if (src.dim() == 3 and not self.norm_first and not self.training and\n",
        "            self.self_attn.batch_first and\n",
        "            self.self_attn._qkv_same_embed_dim and self.activation_relu_or_gelu and\n",
        "            self.norm1.eps == self.norm2.eps and\n",
        "            ((src_mask is None and src_key_padding_mask is None)\n",
        "             if src.is_nested\n",
        "             else (src_mask is None or src_key_padding_mask is None))):\n",
        "            tensor_args = (\n",
        "                src,\n",
        "                self.self_attn.in_proj_weight,\n",
        "                self.self_attn.in_proj_bias,\n",
        "                self.self_attn.out_proj.weight,\n",
        "                self.self_attn.out_proj.bias,\n",
        "                self.norm1.weight,\n",
        "                self.norm1.bias,\n",
        "                self.norm2.weight,\n",
        "                self.norm2.bias,\n",
        "                self.linear1.weight,\n",
        "                self.linear1.bias,\n",
        "                self.linear2.weight,\n",
        "                self.linear2.bias,\n",
        "            )#biases and weights\n",
        "            if (not torch.overrides.has_torch_function(tensor_args) and\n",
        "                    # We have to use a list comprehension here because TorchScript\n",
        "                    # doesn't support generator expressions.\n",
        "                    all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]) and\n",
        "                    (not torch.is_grad_enabled() or all([not x.requires_grad for x in tensor_args]))):\n",
        "                return torch._transformer_encoder_layer_fwd(\n",
        "                    src,\n",
        "                    self.self_attn.embed_dim,\n",
        "                    self.self_attn.num_heads,\n",
        "                    self.self_attn.in_proj_weight,\n",
        "                    self.self_attn.in_proj_bias,\n",
        "                    self.self_attn.out_proj.weight,\n",
        "                    self.self_attn.out_proj.bias,\n",
        "                    self.activation_relu_or_gelu == 2,\n",
        "                    False,\n",
        "                    self.norm1.eps,\n",
        "                    self.norm1.weight,\n",
        "                    self.norm1.bias,\n",
        "                    self.norm2.weight,\n",
        "                    self.norm2.bias,\n",
        "                    self.linear1.weight,\n",
        "                    self.linear1.bias,\n",
        "                    self.linear2.weight,\n",
        "                    self.linear2.bias,\n",
        "                    src_mask if src_mask is not None else src_key_padding_mask,\n",
        "                )\n",
        "        x = src\n",
        "        if self.norm_first:\n",
        "            x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)\n",
        "            x = x + self._ff_block(self.norm2(x))\n",
        "        else:\n",
        "            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))\n",
        "            x = self.norm2(x + self._ff_block(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    # self-attention block\n",
        "    def _sa_block(self, x: Tensor,\n",
        "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
        "        x = self.self_attn(x, x, x,\n",
        "                           attn_mask=attn_mask,\n",
        "                           key_padding_mask=key_padding_mask,\n",
        "                           need_weights=False)[0]\n",
        "        return self.dropout1(x)\n",
        "\n",
        "    # feed forward block\n",
        "    def _ff_block(self, x: Tensor) -> Tensor:\n",
        "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "        return self.dropout2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56307ea5",
      "metadata": {
        "id": "56307ea5"
      },
      "source": [
        "#### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f6187d",
      "metadata": {
        "id": "b0f6187d"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(Module):\n",
        "    __constants__ = ['batch_first', 'norm_first']\n",
        "\n",
        "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
        "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
        "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
        "                 device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "                                            **factory_kwargs)\n",
        "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "                                                 **factory_kwargs)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
        "\n",
        "        self.norm_first = norm_first\n",
        "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "        self.dropout3 = Dropout(dropout)\n",
        "\n",
        "        # Legacy string support for activation function.\n",
        "        if isinstance(activation, str):\n",
        "            self.activation = _get_activation_fn(activation)\n",
        "        else:\n",
        "            self.activation = activation\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(TransformerDecoderLayer, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "\n",
        "        x = tgt\n",
        "        if self.norm_first:\n",
        "            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
        "            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)\n",
        "            x = x + self._ff_block(self.norm3(x))\n",
        "        else:\n",
        "            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
        "            x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))\n",
        "            x = self.norm3(x + self._ff_block(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    # self-attention block\n",
        "    def _sa_block(self, x: Tensor,\n",
        "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
        "        x = self.self_attn(x, x, x,\n",
        "                           attn_mask=attn_mask,\n",
        "                           key_padding_mask=key_padding_mask,\n",
        "                           need_weights=False)[0]\n",
        "        return self.dropout1(x)\n",
        "\n",
        "    # multihead attention block\n",
        "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
        "                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
        "        x = self.multihead_attn(x, mem, mem,\n",
        "                                attn_mask=attn_mask,\n",
        "                                key_padding_mask=key_padding_mask,\n",
        "                                need_weights=False)[0]\n",
        "        return self.dropout2(x)\n",
        "\n",
        "    # feed forward block\n",
        "    def _ff_block(self, x: Tensor) -> Tensor:\n",
        "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "        return self.dropout3(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b32fe0",
      "metadata": {
        "id": "e9b32fe0"
      },
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa953d0d",
      "metadata": {
        "id": "aa953d0d"
      },
      "source": [
        "#### Encoder Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6830bc73",
      "metadata": {
        "id": "6830bc73"
      },
      "outputs": [],
      "source": [
        "# TransformerEncoder is a stack of N encoder layers\n",
        "class TransformerEncoder(Module):\n",
        "\n",
        "    __constants__ = ['norm']\n",
        "\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None, enable_nested_tensor=True):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.enable_nested_tensor = enable_nested_tensor\n",
        "\n",
        "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "        output = src\n",
        "        convert_to_nested = False\n",
        "        first_layer = self.layers[0]\n",
        "        if isinstance(first_layer, torch.nn.TransformerEncoderLayer):\n",
        "            if (not first_layer.norm_first and not first_layer.training and\n",
        "                    first_layer.self_attn.batch_first and\n",
        "                    first_layer.self_attn._qkv_same_embed_dim and first_layer.activation_relu_or_gelu and\n",
        "                    first_layer.norm1.eps == first_layer.norm2.eps and\n",
        "                    src.dim() == 3 and self.enable_nested_tensor) :\n",
        "                if src_key_padding_mask is not None and not output.is_nested and mask is None:\n",
        "                    tensor_args = (\n",
        "                        src,\n",
        "                        first_layer.self_attn.in_proj_weight,\n",
        "                        first_layer.self_attn.in_proj_bias,\n",
        "                        first_layer.self_attn.out_proj.weight,\n",
        "                        first_layer.self_attn.out_proj.bias,\n",
        "                        first_layer.norm1.weight,\n",
        "                        first_layer.norm1.bias,\n",
        "                        first_layer.norm2.weight,\n",
        "                        first_layer.norm2.bias,\n",
        "                        first_layer.linear1.weight,\n",
        "                        first_layer.linear1.bias,\n",
        "                        first_layer.linear2.weight,\n",
        "                        first_layer.linear2.bias,\n",
        "                    )\n",
        "                    if not torch.overrides.has_torch_function(tensor_args):\n",
        "                        if output.is_cuda or 'cpu' in str(output.device):\n",
        "                            convert_to_nested = True\n",
        "                            output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())\n",
        "\n",
        "        for mod in self.layers:\n",
        "            if convert_to_nested:\n",
        "                output = mod(output, src_mask=mask)\n",
        "            else:\n",
        "                output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        if convert_to_nested:\n",
        "            output = output.to_padded_tensor(0.)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0fd56e6",
      "metadata": {
        "id": "a0fd56e6"
      },
      "source": [
        "#### Decoder Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a822c33c",
      "metadata": {
        "id": "a822c33c"
      },
      "outputs": [],
      "source": [
        "# TransformerDecoder is a stack of N Decoder layers\n",
        "class TransformerDecoder(Module):\n",
        "    __constants__ = ['norm']\n",
        "\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "\n",
        "        output = tgt\n",
        "\n",
        "        for mod in self.layers:\n",
        "            output = mod(output, memory, tgt_mask=tgt_mask,\n",
        "                         memory_mask=memory_mask,\n",
        "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                         memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8adbabce",
      "metadata": {
        "id": "8adbabce"
      },
      "source": [
        "#### Transformer Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35078e19",
      "metadata": {
        "id": "35078e19"
      },
      "outputs": [],
      "source": [
        "class Transformer(Module):\n",
        "   #d_model=size of input embeddings is just a vector represenation of the particular word\n",
        "\n",
        "    def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,\n",
        "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
        "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
        "                 custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None,\n",
        "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
        "                 device=None, dtype=None) -> None:\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        if custom_encoder is not None:\n",
        "            self.encoder = custom_encoder\n",
        "        else:\n",
        "            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
        "                                                    activation, layer_norm_eps, batch_first, norm_first,\n",
        "                                                    **factory_kwargs)\n",
        "            encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        if custom_decoder is not None:\n",
        "            self.decoder = custom_decoder\n",
        "        else:\n",
        "            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
        "                                                    activation, layer_norm_eps, batch_first, norm_first,\n",
        "                                                    **factory_kwargs)\n",
        "            decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "\n",
        "# src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "\n",
        "        is_batched = src.dim() == 3\n",
        "        if not self.batch_first and src.size(1) != tgt.size(1) and is_batched:\n",
        "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
        "        elif self.batch_first and src.size(0) != tgt.size(0) and is_batched:\n",
        "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
        "\n",
        "        if src.size(-1) != self.d_model or tgt.size(-1) != self.d_model:\n",
        "            raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
        "\n",
        "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)#encoding forward pass\n",
        "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
        "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                              memory_key_padding_mask=memory_key_padding_mask)#decoding forward pass\n",
        "        return output\n",
        "    #the forward pass of decoder is our output\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
        "\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        \"\"\"Initiate parameters in the transformer model.\"\"\"\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                xavier_uniform_(p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eeffc40",
      "metadata": {
        "id": "8eeffc40"
      },
      "outputs": [],
      "source": [
        "def _get_clones(module, N):\n",
        "    return ModuleList([copy.deepcopy(module) for i in range(N)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c78a87b",
      "metadata": {
        "id": "4c78a87b"
      },
      "source": [
        "#### Calling Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e8878e",
      "metadata": {
        "id": "b2e8878e"
      },
      "outputs": [],
      "source": [
        "def _get_activation_fn(activation: str) -> Callable[[Tensor], Tensor]:\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return F.gelu\n",
        "\n",
        "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86130943",
      "metadata": {
        "id": "86130943"
      },
      "source": [
        "### Reading English Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "474c3f12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "474c3f12",
        "outputId": "b31b58a9-da56-45cf-f662-acbf5ec9f92c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-12-1dd8685580aa>\", line 1, in <cell line: 0>\n",
            "    torch.manual_seed(0)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/random.py\", line 46, in manual_seed\n",
            "    return default_generator.manual_seed(seed)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/random.py:46: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  return default_generator.manual_seed(seed)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Los Angeles has lost night straight and 13 of its first 14 games to start the season.\n",
            "Opposite qualities of meaning of person's name\n",
            "To show anger after getting embarrassed\n",
            "Money earned the wrong way will be taken away\n",
            "To talk big without having a big position\n",
            "More mouths will have more talks\n",
            "To use the available opportunity\n",
            "Getting involved without having\n",
            "The grass is always greener on the other side\n",
            "A person of no principles\n",
            "Division is main reason for the damage\n",
            "Evidence does not need proof\n",
            "A\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "fileEnglish = open('/content/drive/MyDrive/myfolderUTE/English.txt', mode='rt', encoding='utf-8')\n",
        "englishDataset = fileEnglish.read()\n",
        "print(englishDataset[0:500])\n",
        "file = open(\"/content/drive/MyDrive/myfolderUTE/English.txt\", \"r\",encoding='utf-8')\n",
        "x = 0\n",
        "for line in file:\n",
        "\n",
        "    if line != \"\\n\":\n",
        "        x += 1\n",
        "    #print(line)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88976542",
      "metadata": {
        "id": "88976542"
      },
      "source": [
        "### Shape of Urdu Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5787333",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5787333",
        "outputId": "040a7e2e-4c7d-404f-cede-acb5172570f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100000\n"
          ]
        }
      ],
      "source": [
        "fileUrdu = open('/content/drive/MyDrive/myfolderUTE/Urdu.txt', mode='rt', encoding='utf-8')\n",
        "urduDataset = fileUrdu.read()\n",
        "\n",
        "file = open(\"/content/drive/MyDrive/myfolderUTE/Urdu.txt\", \"r\",encoding='utf-8')\n",
        "x = 0\n",
        "for line in file:\n",
        "\n",
        "    if line != \"\\n\":\n",
        "        x += 1\n",
        "print((x))\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3423fe1d",
      "metadata": {
        "id": "3423fe1d"
      },
      "source": [
        "### Data Splitting (70, 15, 15) & Creating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af190132",
      "metadata": {
        "id": "af190132"
      },
      "outputs": [],
      "source": [
        "train_size_en = int(0.70 * x)\n",
        "test_size_en = int(0.15 * x)\n",
        "val_size_en = int(0.15 * x)\n",
        "#division on the basis of lines\n",
        "\n",
        "train_dataset = englishDataset[0:train_size_en]\n",
        "test_dataset = englishDataset[train_size_en+1:train_size_en+test_size_en]\n",
        "val_dataset = englishDataset[train_size_en+test_size_en+1:x]\n",
        "\n",
        "train_size_urdu= int(0.70 * x)\n",
        "test_size_urdu = int(0.15 *x)\n",
        "val_size_urdu = int(0.15 * x)\n",
        "\n",
        "train_dataset2 = urduDataset[0:train_size_urdu]\n",
        "test_dataset2 = urduDataset[train_size_urdu+1:train_size_urdu+test_size_urdu]\n",
        "val_dataset2 = urduDataset[train_size_urdu+test_size_urdu+1:x]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d173eed",
      "metadata": {
        "id": "5d173eed"
      },
      "source": [
        "### Generate Separate Files for Each Split (English)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b307ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "84b307ff",
        "outputId": "0a021874-0d08-497c-af99-83ad49635d23"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nf=open(\"english_train.txt\",\"w\")\\n\\nf.write(train_dataset)\\n\\nf=open(\"english_test.txt\",\"w\")\\nf.write(test_dataset)\\n\\nf=open(\"english_val.txt\",\"w\")\\nf.write(val_dataset)\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "f=open(\"english_train.txt\",\"w\")\n",
        "\n",
        "f.write(train_dataset)\n",
        "\n",
        "f=open(\"english_test.txt\",\"w\")\n",
        "f.write(test_dataset)\n",
        "\n",
        "f=open(\"english_val.txt\",\"w\")\n",
        "f.write(val_dataset)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55166e0c",
      "metadata": {
        "id": "55166e0c"
      },
      "source": [
        "### Generate Separate Files for Each Split (Urdu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b96fc1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1b96fc1c",
        "outputId": "e7196790-51cb-4861-f56b-7af889d77677"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nf=open(\"urdu_train.txt\",\"w\",encoding=\"utf-8\")\\n\\nf.write(train_dataset2)\\n\\nf=open(\"urdu_test.txt\",\"w\",encoding=\"utf-8\")\\nf.write(test_dataset2)\\n\\nf=open(\"urdu_val.txt\",\"w\",encoding=\"utf-8\")\\nf.write(val_dataset2)\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "f=open(\"urdu_train.txt\",\"w\",encoding=\"utf-8\")\n",
        "\n",
        "f.write(train_dataset2)\n",
        "\n",
        "f=open(\"urdu_test.txt\",\"w\",encoding=\"utf-8\")\n",
        "f.write(test_dataset2)\n",
        "\n",
        "f=open(\"urdu_val.txt\",\"w\",encoding=\"utf-8\")\n",
        "f.write(val_dataset2)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "157463fe",
      "metadata": {
        "id": "157463fe"
      },
      "source": [
        "### Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ecf351e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ecf351e",
        "outputId": "24ead2f6-a647-4362-9e94-5dfe8c86e291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab()\n"
          ]
        }
      ],
      "source": [
        "ur_tokenizer = urdu_sen_tokens\n",
        "en_tokenizer = eng_sen_tokens\n",
        "\n",
        "'''\n",
        "get_tokenizer('spacy', language='en_core_web_sm')\n",
        "so the tokenizer here points to the function of tokenizer\n",
        "which then gets passed to build vocab function\n",
        "and in the tokenizer we pass the line\n",
        "'''\n",
        "\n",
        "def build_vocab(filepath, tokenizer):\n",
        "  counter = Counter() # counter builds dictionary of word with its frequencies\n",
        "  with io.open(filepath,encoding=\"utf8\",errors='ignore' ) as f:\n",
        "    for string_ in f:\n",
        "      #print(tokenizer(string_))\n",
        "      counter.update(tokenizer(string_))\n",
        "  return vocab(counter,specials = ['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "main_filepaths = [\"/content/drive/MyDrive/myfolderUTE/English.txt\",\"/content/drive/MyDrive/myfolderUTE/Urdu.txt\"]\n",
        "train_filepaths = [\"/content/drive/MyDrive/myfolderUTE/english_train.txt\",\"/content/drive/MyDrive/myfolderUTE/urdu_train.txt\"]\n",
        "val_filepaths = [\"/content/drive/MyDrive/myfolderUTE/english_val.txt\",\"/content/drive/MyDrive/myfolderUTE/urdu_val.txt\"]\n",
        "test_filepaths = [\"/content/drive/MyDrive/myfolderUTE/english_test.txt\",\"/content/drive/MyDrive/myfolderUTE/urdu_test.txt\"]\n",
        "en_vocab = build_vocab(train_filepaths[0], en_tokenizer)\n",
        "de_vocab = build_vocab(train_filepaths[1], ur_tokenizer)\n",
        "\n",
        "print(de_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "336588bd",
      "metadata": {
        "id": "336588bd"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60e97df1",
      "metadata": {
        "id": "60e97df1"
      },
      "outputs": [],
      "source": [
        "def data_process(filepaths):\n",
        "    raw_de_iter = iter(io.open(filepaths[1], encoding=\"utf8\",errors='ignore'))#created iterator for urdu file\n",
        "    raw_en_iter = iter(io.open(filepaths[0], encoding=\"utf8\",errors='ignore'))#created english iterator for english file\n",
        "    data = []\n",
        "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):#use zip to make alignment between files\n",
        "        #such as Los angeles with los angeles\n",
        "        #raw_de ,raw_en is for accessing that tuple(las angeles in urdu,los angeles in englus)\n",
        "        de_tensor_ = torch.tensor([de_vocab[token] for token in ur_tokenizer(raw_de.rstrip(\"\\n\"))],\n",
        "                                dtype=torch.long)\n",
        "        #full urdu sentence with \\n where the eol character is remover using de_strip is tokenized and each token is search in the vocabulary\n",
        "        #whereas the voabulary is containing the embedding on the basis of frequency\n",
        "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"\\n\"))],\n",
        "                                dtype=torch.long)\n",
        "        data.append((de_tensor_, en_tensor_))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89d9c378",
      "metadata": {
        "id": "89d9c378"
      },
      "source": [
        "### Map Train, Test & Validation on Data Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33915bab",
      "metadata": {
        "id": "33915bab"
      },
      "outputs": [],
      "source": [
        "de_vocab.set_default_index(0)\n",
        "en_vocab.set_default_index(0) #if no index is found then 0 is returned\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c8c0dd0",
      "metadata": {
        "id": "5c8c0dd0"
      },
      "source": [
        "### Setting Batch Size & Special Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5050ad1",
      "metadata": {
        "id": "f5050ad1"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "PAD_IDX = de_vocab['<pad>'] #padding in sentence\n",
        "BOS_IDX = de_vocab['<bos>'] #beggining of sentence\n",
        "EOS_IDX = de_vocab['<eos>'] #representing end of sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922ca43c",
      "metadata": {
        "id": "922ca43c"
      },
      "source": [
        "### Creating Dataloaders for Each Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56741ad3",
      "metadata": {
        "id": "56741ad3"
      },
      "outputs": [],
      "source": [
        "# basically concatenatnation start , end, pad and the tensor to make a element and append it in the batch\n",
        "def generate_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    # torch.cat concatenates the tensors\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return de_batch, en_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch) # dividing the data in batches with the batch size specified\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c7de95e",
      "metadata": {
        "id": "0c7de95e"
      },
      "source": [
        "### TokenEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f6c2654",
      "metadata": {
        "id": "2f6c2654"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96556cf3",
      "metadata": {
        "id": "96556cf3"
      },
      "source": [
        "### PositionalEncoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9baf27af",
      "metadata": {
        "id": "9baf27af"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Values for Positional Encoding PE(pos,i)=sin(pos/10000**2i/d)\n",
        "i is the index of the word\n",
        "and pos is the position\n",
        "where d=size of embeddings\n",
        "pos 0 means the first positional embedding\n",
        "pos 1 means the 2nd and so on\n",
        "and i is the index in position embedding we are filling\n",
        "'''\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding +\n",
        "                            self.pos_embedding[:token_embedding.size(0),:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48da88f7",
      "metadata": {
        "id": "48da88f7"
      },
      "source": [
        "### Seq2Seq Transformer Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a4f885",
      "metadata": {
        "id": "19a4f885"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
        "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
        "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
        "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        #print(src_emb)\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        #print(tgt-_emb)\n",
        "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
        "        #print(memory)\n",
        "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
        "                                        tgt_padding_mask, memory_key_padding_mask)\n",
        "        #print(outs)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer_encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer_decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73f7574",
      "metadata": {
        "id": "e73f7574"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c1861d7",
      "metadata": {
        "id": "2c1861d7"
      },
      "source": [
        "### Defining Model Parameters and Instantiate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63197d49",
      "metadata": {
        "id": "63197d49"
      },
      "outputs": [],
      "source": [
        "SRC_VOCAB_SIZE = len(de_vocab)\n",
        "TGT_VOCAB_SIZE = len(en_vocab)\n",
        "\n",
        "EMB_SIZE = 512\n",
        "\n",
        "NHEAD = 8\n",
        "\n",
        "FFN_HID_DIM = 512\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "NUM_EPOCHS = 16\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
        "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
        "                                 FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b6d06e1",
      "metadata": {
        "id": "7b6d06e1"
      },
      "source": [
        "### Training Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b58521",
      "metadata": {
        "id": "b8b58521"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_iter, optimizer):\n",
        "  model.train()\n",
        "  losses = 0\n",
        "  for idx, (src, tgt) in enumerate(train_iter):\n",
        "\n",
        "      src = src.to(device)\n",
        "      tgt = tgt.to(device)\n",
        "\n",
        "      tgt_input = tgt[:-1, :]\n",
        "\n",
        "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "      logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      tgt_out = tgt[1:,:]\n",
        "      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      losses += loss.item()\n",
        "  return losses / len(train_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cefc1b80",
      "metadata": {
        "id": "cefc1b80"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee4b153",
      "metadata": {
        "id": "aee4b153"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, val_iter):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    for idx, (src, tgt) in (enumerate(valid_iter)):\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        tgt_out = tgt[1:,:]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "    return losses / len(val_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3c742dc",
      "metadata": {
        "id": "e3c742dc"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5218fbfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5218fbfc",
        "outputId": "678fc7f3-c424-448a-f183-35a08fe416b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train loss: 0.037, Val loss: 4.791, Epoch time = 0.902s\n",
            "Epoch: 2, Train loss: 0.036, Val loss: 4.791, Epoch time = 0.903s\n",
            "Epoch: 3, Train loss: 0.032, Val loss: 4.816, Epoch time = 0.913s\n",
            "Epoch: 4, Train loss: 0.033, Val loss: 4.821, Epoch time = 0.961s\n",
            "Epoch: 5, Train loss: 0.031, Val loss: 4.789, Epoch time = 0.911s\n",
            "Epoch: 6, Train loss: 0.032, Val loss: 4.827, Epoch time = 0.904s\n",
            "Epoch: 7, Train loss: 0.033, Val loss: 4.756, Epoch time = 1.159s\n",
            "Epoch: 8, Train loss: 0.029, Val loss: 4.819, Epoch time = 1.647s\n",
            "Epoch: 9, Train loss: 0.033, Val loss: 4.702, Epoch time = 1.815s\n",
            "Epoch: 10, Train loss: 0.027, Val loss: 4.800, Epoch time = 1.403s\n",
            "Epoch: 11, Train loss: 0.031, Val loss: 4.817, Epoch time = 0.917s\n",
            "Epoch: 12, Train loss: 0.028, Val loss: 4.918, Epoch time = 0.980s\n",
            "Epoch: 13, Train loss: 0.028, Val loss: 4.880, Epoch time = 0.906s\n",
            "Epoch: 14, Train loss: 0.028, Val loss: 4.790, Epoch time = 0.908s\n",
            "Epoch: 15, Train loss: 0.031, Val loss: 4.833, Epoch time = 0.890s\n",
            "Epoch: 16, Train loss: 0.029, Val loss: 4.893, Epoch time = 0.902s\n",
            "Epoch: 17, Train loss: 0.030, Val loss: 4.845, Epoch time = 0.908s\n",
            "Epoch: 18, Train loss: 0.028, Val loss: 4.871, Epoch time = 0.904s\n",
            "Epoch: 19, Train loss: 0.029, Val loss: 4.800, Epoch time = 0.905s\n",
            "Epoch: 20, Train loss: 0.029, Val loss: 4.850, Epoch time = 0.912s\n",
            "Epoch: 21, Train loss: 0.025, Val loss: 4.941, Epoch time = 1.440s\n",
            "Epoch: 22, Train loss: 0.027, Val loss: 5.012, Epoch time = 1.478s\n",
            "Epoch: 23, Train loss: 0.025, Val loss: 5.013, Epoch time = 0.919s\n",
            "Epoch: 24, Train loss: 0.025, Val loss: 4.995, Epoch time = 0.906s\n",
            "Epoch: 25, Train loss: 0.027, Val loss: 5.139, Epoch time = 0.908s\n",
            "Epoch: 26, Train loss: 0.028, Val loss: 5.135, Epoch time = 0.893s\n",
            "Epoch: 27, Train loss: 0.026, Val loss: 5.002, Epoch time = 0.885s\n",
            "Epoch: 28, Train loss: 0.027, Val loss: 5.052, Epoch time = 0.888s\n",
            "Epoch: 29, Train loss: 0.027, Val loss: 5.035, Epoch time = 0.901s\n",
            "Epoch: 30, Train loss: 0.024, Val loss: 4.945, Epoch time = 0.908s\n",
            "Epoch: 31, Train loss: 0.024, Val loss: 5.084, Epoch time = 0.943s\n",
            "Epoch: 32, Train loss: 0.024, Val loss: 5.053, Epoch time = 0.939s\n",
            "Epoch: 33, Train loss: 0.026, Val loss: 5.182, Epoch time = 1.395s\n",
            "Epoch: 34, Train loss: 0.026, Val loss: 5.172, Epoch time = 1.404s\n",
            "Epoch: 35, Train loss: 0.024, Val loss: 5.000, Epoch time = 1.091s\n",
            "Epoch: 36, Train loss: 0.025, Val loss: 4.973, Epoch time = 0.904s\n",
            "Epoch: 37, Train loss: 0.024, Val loss: 5.069, Epoch time = 0.907s\n",
            "Epoch: 38, Train loss: 0.024, Val loss: 5.145, Epoch time = 0.901s\n",
            "Epoch: 39, Train loss: 0.023, Val loss: 5.079, Epoch time = 0.914s\n",
            "Epoch: 40, Train loss: 0.024, Val loss: 5.039, Epoch time = 0.972s\n",
            "Epoch: 41, Train loss: 0.022, Val loss: 5.343, Epoch time = 0.933s\n",
            "Epoch: 42, Train loss: 0.022, Val loss: 5.208, Epoch time = 0.891s\n",
            "Epoch: 43, Train loss: 0.024, Val loss: 5.164, Epoch time = 0.905s\n",
            "Epoch: 44, Train loss: 0.026, Val loss: 5.348, Epoch time = 0.880s\n",
            "Epoch: 45, Train loss: 0.023, Val loss: 5.279, Epoch time = 1.165s\n",
            "Epoch: 46, Train loss: 0.024, Val loss: 5.243, Epoch time = 1.373s\n",
            "Epoch: 47, Train loss: 0.022, Val loss: 5.401, Epoch time = 1.190s\n",
            "Epoch: 48, Train loss: 0.023, Val loss: 5.206, Epoch time = 0.889s\n",
            "Epoch: 49, Train loss: 0.023, Val loss: 5.393, Epoch time = 0.901s\n",
            "Epoch: 50, Train loss: 0.023, Val loss: 5.459, Epoch time = 0.877s\n",
            "Epoch: 51, Train loss: 0.026, Val loss: 5.153, Epoch time = 0.873s\n",
            "Epoch: 52, Train loss: 0.025, Val loss: 5.350, Epoch time = 0.897s\n",
            "Epoch: 53, Train loss: 0.024, Val loss: 5.427, Epoch time = 0.903s\n",
            "Epoch: 54, Train loss: 0.022, Val loss: 5.326, Epoch time = 0.918s\n",
            "Epoch: 55, Train loss: 0.021, Val loss: 5.341, Epoch time = 0.919s\n",
            "Epoch: 56, Train loss: 0.024, Val loss: 5.468, Epoch time = 0.907s\n",
            "Epoch: 57, Train loss: 0.021, Val loss: 5.523, Epoch time = 1.045s\n",
            "Epoch: 58, Train loss: 0.022, Val loss: 5.454, Epoch time = 1.403s\n",
            "Epoch: 59, Train loss: 0.023, Val loss: 5.399, Epoch time = 1.354s\n",
            "Epoch: 60, Train loss: 0.024, Val loss: 5.231, Epoch time = 0.905s\n",
            "Epoch: 61, Train loss: 0.022, Val loss: 5.450, Epoch time = 0.918s\n",
            "Epoch: 62, Train loss: 0.021, Val loss: 5.496, Epoch time = 0.883s\n",
            "Epoch: 63, Train loss: 0.022, Val loss: 5.166, Epoch time = 0.881s\n",
            "Epoch: 64, Train loss: 0.023, Val loss: 5.508, Epoch time = 0.889s\n",
            "Epoch: 65, Train loss: 0.022, Val loss: 5.613, Epoch time = 0.899s\n",
            "Epoch: 66, Train loss: 0.022, Val loss: 5.723, Epoch time = 0.884s\n",
            "Epoch: 67, Train loss: 0.021, Val loss: 5.942, Epoch time = 0.878s\n",
            "Epoch: 68, Train loss: 0.024, Val loss: 5.967, Epoch time = 0.883s\n",
            "Epoch: 69, Train loss: 0.020, Val loss: 5.923, Epoch time = 0.919s\n",
            "Epoch: 70, Train loss: 0.021, Val loss: 5.730, Epoch time = 1.396s\n",
            "Epoch: 71, Train loss: 0.025, Val loss: 5.628, Epoch time = 1.440s\n",
            "Epoch: 72, Train loss: 0.022, Val loss: 6.067, Epoch time = 0.947s\n",
            "Epoch: 73, Train loss: 0.022, Val loss: 6.177, Epoch time = 0.865s\n",
            "Epoch: 74, Train loss: 0.021, Val loss: 6.234, Epoch time = 0.902s\n",
            "Epoch: 75, Train loss: 0.023, Val loss: 6.204, Epoch time = 0.889s\n",
            "Epoch: 76, Train loss: 0.020, Val loss: 6.249, Epoch time = 0.890s\n",
            "Epoch: 77, Train loss: 0.022, Val loss: 6.523, Epoch time = 0.905s\n",
            "Epoch: 78, Train loss: 0.021, Val loss: 6.256, Epoch time = 0.903s\n",
            "Epoch: 79, Train loss: 0.022, Val loss: 6.358, Epoch time = 0.912s\n",
            "Epoch: 80, Train loss: 0.021, Val loss: 6.281, Epoch time = 0.910s\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 80\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = time.time()\n",
        "    train_loss = train_epoch(transformer, train_iter, optimizer)\n",
        "    end_time = time.time()\n",
        "    val_loss = evaluate(transformer, valid_iter)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
        "          f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5193f58",
      "metadata": {
        "id": "a5193f58"
      },
      "source": [
        "### Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eaec500",
      "metadata": {
        "id": "1eaec500"
      },
      "outputs": [],
      "source": [
        "def calc_bleu_score(translations, references):\n",
        "    translations_formatted = [translation.split() for translation in translations]\n",
        "    references_formatted = [[translation.split()] for translation in references]\n",
        "    return bleu_score(translations_formatted, references_formatted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336fd1ad",
      "metadata": {
        "id": "336fd1ad"
      },
      "outputs": [],
      "source": [
        "def calc_bleu_score_from_file(filename):\n",
        "  with open(f'/content/drive/MyDrive/myfolderUTE/english_train.txt') as file:\n",
        "      translations = file.readlines()\n",
        "\n",
        "  return calc_bleu_score(translations=translations, references=en_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3904109d",
      "metadata": {
        "id": "3904109d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)\n",
        "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                                    .type(torch.bool)).to(device)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M5EEI416Vleh",
      "metadata": {
        "id": "M5EEI416Vleh"
      },
      "outputs": [],
      "source": [
        "'''def greedy_decode(model, src, src_mask, max_len, start_symbol, beam_width=5):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "\n",
        "    # Beam search implementation\n",
        "    candidates = [(ys, 0)]  # (sequence, score)\n",
        "\n",
        "    for _ in range(max_len-1):\n",
        "        new_candidates = []\n",
        "        for seq, score in candidates:\n",
        "            if seq[-1] == EOS_IDX:\n",
        "                new_candidates.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            memory_mask = torch.zeros(seq.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "            tgt_mask = generate_square_subsequent_mask(seq.size(0)).type(torch.bool).to(device)\n",
        "\n",
        "            out = model.decode(seq, memory, tgt_mask)\n",
        "            out = out.transpose(0, 1)\n",
        "            logits = model.generator(out[:, -1])\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            top_probs, top_idx = probs.topk(beam_width)\n",
        "            for i in range(beam_width):\n",
        "                new_seq = torch.cat([seq, top_idx[0][i].unsqueeze(0).unsqueeze(0)], dim=0)\n",
        "                new_score = score + torch.log(top_probs[0][i]).item()\n",
        "                new_candidates.append((new_seq, new_score))\n",
        "\n",
        "        # Keep top beam_width candidates\n",
        "        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "    return candidates[0][0]'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a059c915",
      "metadata": {
        "id": "a059c915"
      },
      "source": [
        "### Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f680d6",
      "metadata": {
        "id": "58f680d6"
      },
      "outputs": [],
      "source": [
        "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
        "    model.eval()\n",
        "    tokens = [BOS_IDX] + [src_vocab.get_stoi()[tok] for tok in src_tokenizer(src)]+ [EOS_IDX]\n",
        "    num_tokens = len(tokens)\n",
        "    src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join([tgt_vocab.get_itos()[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e952c12a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e952c12a",
        "outputId": "498908dc-9c65-45c4-f0a2-4b42ba124fb6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' More mouths will have more talks '"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(transformer, \"جتنے منہ اتنی باتیں\", de_vocab, en_vocab, ur_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3357245d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3357245d",
        "outputId": "59b26fdc-06c2-417b-fe5b-ceb83eccff5d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Birds of same feather flock together '"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(transformer, \"چور چور مسیرے بھائی\", de_vocab, en_vocab, ur_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3380cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ff3380cb",
        "outputId": "b5bdc88c-d397-433f-d836-13a255b2cd0a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' A figure among cyphers '"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(transformer, \"اندھوں میں کانا راجہ\", de_vocab, en_vocab, ur_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0771c71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a0771c71",
        "outputId": "70e931d7-bfee-4d27-c2bb-48eb5ec784ac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Success is not final, failure is not fatal: it is the courage to continue that counts '"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(transformer, \"نہ کامیابی حتمی ہوتی ہے اور نہ ہی ناکامی: بلکہ اصل چیز کوشش جاری رکھنے کا حوصلہ ہوتا ہے\", de_vocab, en_vocab, ur_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a0a0ae5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a0a0ae5",
        "outputId": "9b86d9ca-edcc-4eb8-d7f9-f6cd2e1abd5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "لاس اینجلس نے سیزن شروع کرنے کے لئے سیدھے رات اور اپنے پہلے 14 میں سے 13 کھیل کھوئے ہیں۔\n",
            " Los Angeles has lost night straight and 13 of its first 14 games to start the season. \n",
            "آنکھ کا اندھا نام نین سکھ\n",
            " Opposite qualities of meaning of person's name \n",
            "کھسیانی بلی کھمبا نوچے\n",
            " To show anger after getting embarrassed \n",
            "چوری کا مال موری میں\n",
            " Money earned the wrong way will be taken away \n",
            "چھوٹا منہ بڑی بات\n",
            " To talk big without having a big position \n",
            "جتنے منہ اتنی باتیں\n",
            " More mouths will have more talks \n",
            "بہتی گنگا میں ہاتھ دھونا\n",
            " To use the available opportunity \n",
            "مان نہ مان میں تیرا مہمان\n",
            " An uninvited guest is never welcomed \n",
            "دور کے ڈھول سُہانے\n",
            " The grass is always greener on the other side \n",
            "گنگا گائے گنگا داس جمنا گائے جمنا داس\n",
            " A person of no principles \n",
            "گھر کا بھیدی لنکا ڈھائے\n",
            " Division is main reason for the damage \n",
            "ہاتھ کنگن کو آرسی کیا\n",
            " Evidence does not need proof \n",
            "دھوبی کا کتا نہ گھر کا نہ گھاٹ کا\n",
            " A person try to be on two sides goes nowhere \n",
            "انگور کھٹے ہیں\n",
            " Sour grapes \n",
            "دال میں کالا\n",
            " More to it than meets the eye \n",
            "ناچ نہ جانے آنگھن ٹیڑھا\n",
            " A poor worker blames his tools \n",
            "جلے پر نمک چھڑکنا\n",
            " Rubbing salt on one's wound \n",
            "آگے کنواں پیچھے کھائی\n",
            " Between the devil and the deep sea \n",
            "جتنی چادر ہو اتنا پیر پھیلاو\n",
            " Cut your coat according to your cloth \n",
            "اب پچھتائے کیا، جب چڑیاں چگ گئیں کھیت\n",
            " No use crying over spilt milk \n",
            "انتھ بھلا تو سب بھلا\n",
            " All is well that ends well \n",
            "دودھ کا جلا چھاچھ بھی پھونک کر پیتا ہے\n",
            " Once bitten twice shy \n",
            "جیسا دیس ویسا بھیس\n",
            " In Rome do as the Romans do \n",
            "ایک میان میں دو تلواریں نہیں سماتیں\n",
            " No man can serve two masters \n",
            "کھوٹا چھنا باجے گھنا\n",
            " Empty vessels make more noise \n"
          ]
        }
      ],
      "source": [
        "# Using readlines()\n",
        "file1 = open('/content/drive/MyDrive/myfolderUTE/urdu_train.txt', 'r',encoding='utf8')\n",
        "Lines = file1.readlines()\n",
        "\n",
        "file2=open('trans.txt','w',encoding='utf8')\n",
        "count = 0\n",
        "# Strips the newline character\n",
        "for line in Lines:\n",
        "    count += 1\n",
        "    print(line.strip())\n",
        "    var=translate(transformer, line.strip(), de_vocab, en_vocab, ur_tokenizer)\n",
        "    print(var)\n",
        "    file2.writelines(var)\n",
        "    if count==25:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3bd4147",
      "metadata": {
        "id": "a3bd4147"
      },
      "outputs": [],
      "source": [
        "def bleu(data, model, urdu, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate(model, src, urdu, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d54f74",
      "metadata": {
        "collapsed": true,
        "id": "65d54f74"
      },
      "outputs": [],
      "source": [
        "#score = bleu(test_dataset, transformer, ur_tokenizer, en_tokenizer, DEVICE)\n",
        "#print(f\"Bleu score {score:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "omgFnMfQUNtf",
      "metadata": {
        "id": "omgFnMfQUNtf"
      },
      "source": [
        "## Verifying Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J3C1mQMHTyyi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3C1mQMHTyyi",
        "outputId": "098d73ad-80eb-47c5-9343-53697e0247ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pair 1:\n",
            "Urdu: لاس اینجلس نے سیزن شروع کرنے کے لئے سیدھے رات اور اپنے پہلے 14 میں سے 13 کھیل کھوئے ہیں۔\n",
            "English: Los Angeles has lost night straight and 13 of its first 14 games to start the season.\n",
            "-----\n",
            "Pair 2:\n",
            "Urdu: آنکھ کا اندھا نام نین سکھ\n",
            "English: Opposite qualities of meaning of person's name\n",
            "-----\n",
            "Pair 3:\n",
            "Urdu: کھسیانی بلی کھمبا نوچے\n",
            "English: To show anger after getting embarrassed\n",
            "-----\n",
            "Pair 4:\n",
            "Urdu: چوری کا مال موری میں\n",
            "English: Money earned the wrong way will be taken away\n",
            "-----\n",
            "Pair 5:\n",
            "Urdu: چھوٹا منہ بڑی بات\n",
            "English: To talk big without having a big position\n",
            "-----\n",
            "Pair 6:\n",
            "Urdu: جتنے منہ اتنی باتیں\n",
            "English: More mouths will have more talks\n",
            "-----\n",
            "Pair 7:\n",
            "Urdu: بہتی گنگا میں ہاتھ دھونا\n",
            "English: To use the available opportunity\n",
            "-----\n",
            "Pair 8:\n",
            "Urdu: مان نہ مان میں تیرا مہمان\n",
            "English: Getting involved without having\n",
            "-----\n",
            "Pair 9:\n",
            "Urdu: دور کے ڈھول سُہانے\n",
            "English: The grass is always greener on the other side\n",
            "-----\n",
            "Pair 10:\n",
            "Urdu: گنگا گائے گنگا داس جمنا گائے جمنا داس\n",
            "English: A person of no principles\n",
            "-----\n",
            "Pair 11:\n",
            "Urdu: گھر کا بھیدی لنکا ڈھائے\n",
            "English: Division is main reason for the damage\n",
            "-----\n",
            "Pair 12:\n",
            "Urdu: ہاتھ کنگن کو آرسی کیا\n",
            "English: Evidence does not need proof\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "# Verify dataset alignment\n",
        "with open('/content/drive/MyDrive/myfolderUTE/urdu_train.txt', 'r', encoding='utf-8') as f1, \\\n",
        "     open('/content/drive/MyDrive/myfolderUTE/english_train.txt', 'r', encoding='utf-8') as f2:\n",
        "    for i, (urdu, eng) in enumerate(zip(f1, f2)):\n",
        "        print(f\"Pair {i+1}:\")\n",
        "        print(f\"Urdu: {urdu.strip()}\")\n",
        "        print(f\"English: {eng.strip()}\")\n",
        "        print(\"-----\")\n",
        "        if i > 10: break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8vkiX6QBUTeF",
      "metadata": {
        "id": "8vkiX6QBUTeF"
      },
      "source": [
        "## NLTK for METEOR Metric and BLEU Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "em18HS2eOmho",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em18HS2eOmho",
        "outputId": "ac21060a-1a01-4af1-be7d-83f48cdfde45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VYk-djg_OkKA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYk-djg_OkKA",
        "outputId": "7d5cb6a4-0df5-47ce-c16c-5f402008481b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First, let's install required packages for METEOR\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9WhOH3-oaoWT",
      "metadata": {
        "id": "9WhOH3-oaoWT"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from torchtext.data.metrics import bleu_score as torchtext_bleu  # Renamed import\n",
        "\n",
        "def translate_with_fallback(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
        "    model.eval()\n",
        "    # Handle unknown words by using <unk> token\n",
        "    tokens = [BOS_IDX] + [src_vocab.get_stoi().get(tok, src_vocab['<unk>']) for tok in src_tokenizer(src)] + [EOS_IDX]\n",
        "    num_tokens = len(tokens)\n",
        "    src = (torch.LongTensor(tokens).reshape(num_tokens, 1))\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join([tgt_vocab.get_itos()[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "\n",
        "def calculate_bleu_and_meteor(num_examples=25):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    meteor_scores = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/myfolderUTE/urdu_test.txt', 'r', encoding='utf-8') as urdu_file, \\\n",
        "         open('/content/drive/MyDrive/myfolderUTE/english_test.txt', 'r', encoding='utf-8') as english_file:\n",
        "\n",
        "        urdu_lines = urdu_file.readlines()[:num_examples]\n",
        "        english_lines = english_file.readlines()[:num_examples]\n",
        "\n",
        "        for urdu_line, english_line in zip(urdu_lines, english_lines):\n",
        "            try:\n",
        "                # Clean lines\n",
        "                urdu_text = urdu_line.strip()\n",
        "                english_text = english_line.strip()\n",
        "\n",
        "                if not urdu_text or not english_text:\n",
        "                    continue\n",
        "\n",
        "                # Get translation with fallback for unknown words\n",
        "                translated_text = translate_with_fallback(\n",
        "                    transformer,\n",
        "                    urdu_text,\n",
        "                    de_vocab,\n",
        "                    en_vocab,\n",
        "                    ur_tokenizer\n",
        "                )\n",
        "\n",
        "                # Tokenize for metrics\n",
        "                ref_tokens = word_tokenize(english_text.lower())\n",
        "                hyp_tokens = word_tokenize(translated_text.lower())\n",
        "\n",
        "                references.append([ref_tokens])\n",
        "                hypotheses.append(hyp_tokens)\n",
        "\n",
        "                # Calculate METEOR\n",
        "                meteor_scores.append(meteor_score(\n",
        "                    [ref_tokens],\n",
        "                    hyp_tokens\n",
        "                ))\n",
        "\n",
        "                # Print some examples\n",
        "                print(f\"Urdu: {urdu_text}\")\n",
        "                print(f\"Reference: {english_text}\")\n",
        "                print(f\"Translation: {translated_text}\")\n",
        "                print(\"-----\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing: {urdu_text}\")\n",
        "                print(f\"Error: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    # Calculate metrics\n",
        "    bleu = torchtext_bleu(hypotheses, references)  # Use the renamed import\n",
        "    avg_meteor = np.mean(meteor_scores) if meteor_scores else 0\n",
        "\n",
        "    return bleu, avg_meteor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jl_4iLvMOqH_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl_4iLvMOqH_",
        "outputId": "2d46a12c-e600-41ea-da4e-073d70eba1db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Urdu: جیتے ہیں ۔\n",
            "Reference: wing your seed in the sea.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: نُکتہ چینی آسان ہے ۔\n",
            "Reference: Do not do today what you will repent of tomorrow.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: ہر نسل میں نقائص ہوتے ہیں ۔\n",
            "Reference: Do not effusively offer your right hand to everyone.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: ہر مفکر و دانشور اچھا اُستاد نہیں ہو سکتا ۔\n",
            "Reference: Do not entrust your all to one vessel.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: کونسا گلزار ہے جِس میں خزاں آئی نہ ہو ۔\n",
            "Reference: Do not expect friends to do for you what you can do for yourslef.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: ہر طرف ۔ بکھرا ہوا ۔\n",
            "Reference: Do not fan the dying embers.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: ہر کمالے را زوال ! ۔\n",
            "Reference: Do not fight against two adversaries.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: اپنا بار سب کو گراں لگتا ہے ۔\n",
            "Reference: Do not flee conversation, nor let your door be always shut.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: مکمل طور پر  ۔\n",
            "Reference: Do not give a sword to a child.\n",
            "Translation:  Dead as a herring. \n",
            "-----\n",
            "Urdu: خوشی کے ساتھ غم وابستہ ہے ۔\n",
            "Reference: Do not give me words instead of meal.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: پاگل کو سب پاگل ہی لگتے ہیں ۔\n",
            "Reference: Do not go to the council-hall before you are called.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: ہر مُلک کی پیداوار جُدا جُدا ہے ۔\n",
            "Reference: Do not judge the horse by the harness.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: بُہادر کی ہر جگہ عِزَّت ہوتی ہے ۔\n",
            "Reference: Do not keep a dog and bark yourslef.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: ہر قانون میں نقص ہوتے ہیں ۔\n",
            "Reference: Do not kick the ladder by which you climbed.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: چراغ تلے اندھیرا ۔\n",
            "Reference: Do not leave the right path and you are safe.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: ہر جگنو تارا نہیں ہوتا ۔\n",
            "Reference: Do not lose the time in praying.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: تھوڑے کو حقیر نہ جانو ! ۔\n",
            "Reference: Do not lose your friend for your jest.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: دوسرے کا کام سب کو آسان نظر آتا ہے ۔\n",
            "Reference: Do not pick holes in other,s coats.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: اپنی مدد آپ کرو ۔\n",
            "Reference: Do not play with edged tools.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: عیب سے کون پاک ہے ۔\n",
            "Reference: Do not pluck the beard of a dead lion.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: مُصیبت کِس پر نہیں آتی ۔\n",
            "Reference: Do not poke into the rest of hornets.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: کسی کے نصیب کا کوئی ساتھی نہیں ۔\n",
            "Reference: Do not pull all your eggs in one basket.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: اپنے نفع کے لئے انسان کیا کُچھ نہیں کرتا ؟\n",
            "Reference: Do not pull of till tomorrow what you can do today.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: تیس برس کی عمر کے بعد آدمی یا تو بے وقوف ثابت ہوتا ہے یا حکیم بن جاتا ہے ۔\n",
            "Reference: Do not put off till tomorrow what u can do today.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "Urdu: انسان آپ ہی اپنا دُشمن ہے ۔\n",
            "Reference: Do not put off till tomorrow what you can do today.\n",
            "Translation:  Uneasy lies the head that wears a crown \n",
            "-----\n",
            "\n",
            "Final Scores:\n",
            "BLEU Score: 0.0000\n",
            "METEOR Score: 0.0340\n"
          ]
        }
      ],
      "source": [
        "# Calculate metrics\n",
        "final_bleu, final_meteor = calculate_bleu_and_meteor(num_examples=25)  # Changed variable names\n",
        "\n",
        "print(\"\\nFinal Scores:\")\n",
        "print(f\"BLEU Score: {final_bleu:.4f}\")\n",
        "print(f\"METEOR Score: {final_meteor:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
